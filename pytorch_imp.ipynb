{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(df,max_len_synopsis):\n",
    "    fin_text=df.iloc[0]['synopsis']\n",
    "    max_len_synopsis = max(max_len_synopsis, len(fin_text))\n",
    "    chars_temp = set(fin_text)\n",
    "    #chars.union(chars_temp)\n",
    "    for review in df.iloc[0]['reviews']:\n",
    "        chars_temp=chars_temp.union(set(review))\n",
    "    return chars_temp,max_len_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1\n",
    "fin_text=\"\"\n",
    "chars = set(fin_text)\n",
    "max_len_synop=0\n",
    "for chunk in pd.read_csv('preprocessed.tsv', chunksize=chunksize, delimiter='\\t'):\n",
    "    df= pd.DataFrame(chunk)\n",
    "    df['reviews'] = df['reviews'].apply(literal_eval)\n",
    "    temp=func(df,max_len_synop)\n",
    "    chars = chars.union(temp[0])\n",
    "    max_len_synop=max(max_len_synop,temp[1])\n",
    "chars_tuple = tuple(chars)\n",
    "int2char = dict(enumerate(chars_tuple))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', 'm', 'P', 'K', 'F', 'f', '4', '#', '7', 'B', '6', '*', \"'\", 'W', 'l', 'C', 'S', 'Q', '-', 'p', 'b', '8', '2', '=', '(', ' ', 'u', '\"', 'T', 'J', 'ê', ',', 'R', ']', 'g', '?', 'L', '`', 'O', 'H', 'h', 'n', 't', ')', 'V', 'c', 'e', 'D', 'j', '%', 'y', '0', 'U', '~', 'k', ':', 'a', 'z', '$', '/', 'v', 'd', 'A', 'E', 'M', 'Z', 'q', 'N', '&', 's', '[', '9', '3', '\\n', 'é', '+', '!', '\\x96', 'X', 'r', 'w', '.', '5', ';', 'I', 'x', 'o', 'Y', 'G', 'i'}\n",
      "11801\n"
     ]
    }
   ],
   "source": [
    "print(chars)\n",
    "print(max_len_synop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('preprocessed.tsv',delimiter='\\t', nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_review = len(df.iloc[0]['reviews'][0])\n",
    "fortyforty = (int)((len_of_review -35)/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_review = \" \"\n",
    "global_synop = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len_synopsis=0\n",
    "# max_len_reviews=0 \n",
    "# no_of_reviews=0\n",
    "# tot_no_of_reviews=0\n",
    "# fin_text=''\n",
    "# for index,row in df.iterrows():\n",
    "#     max_len_synopsis = max(max_len_synopsis, len(row['synopsis']))\n",
    "#     text=''.join(row['synopsis'])\n",
    "#     fin_text+=' '+text\n",
    "# for i in range(0, len(df)):\n",
    "#     temp = df.iloc[i]['reviews']\n",
    "#     no_of_reviews=0;\n",
    "#     for text in temp:\n",
    "#         max_len_reviews = max(max_len_reviews, len(text))\n",
    "#         no_of_reviews=no_of_reviews+1\n",
    "#         fin_text+=' '+text\n",
    "#     tot_no_of_reviews += no_of_reviews\n",
    "# print(tot_no_of_reviews)\n",
    "# chars = tuple(set(fin_text))\n",
    "# int2char = dict(enumerate(chars))\n",
    "# char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "# fortyforty = (int)((max_len_reviews-35)/5)\n",
    "\n",
    "# #print(fortyforty)\n",
    "# max_len_reviews = 36+5*fortyforty\n",
    "# #print(max_len_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(synopsis, rev40):\n",
    "#     print(max_len_synop)\n",
    "    encoded = np.array([char2int[ch] for ch in synopsis])\n",
    "    encoded2 = np.array([char2int[ch] for ch in rev40])\n",
    "    ans = np.zeros((max_len_synop+40, len(int2char)), dtype = np.float32)\n",
    "    cnt=max_len_synop-len(synopsis)\n",
    "    for x in encoded:\n",
    "#         print(x)\n",
    "        ans[cnt][x] = 1.0\n",
    "        cnt=cnt+1\n",
    "    for x in encoded2:\n",
    "#         print(x)\n",
    "        ans[cnt][x] = 1.0\n",
    "        cnt=cnt+1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encoder(arr, n_labels):\n",
    "    \n",
    "#     # Initialize the the encoded array\n",
    "#     one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "#     # Fill the appropriate elements with ones\n",
    "#     one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "#     # Finally reshape it to get back to the original array\n",
    "#     one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "#     return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_encoding2arr(c):\n",
    "    encoded = np.zeros(len(int2char), dtype = np.float32)\n",
    "    encoded[char2int[c]]=1.0\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(40+max_len_synop, len(int2char))))\n",
    "model.add(Dense(len(int2char)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(global_review) - 40- 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = global_review[start_index: start_index +40]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(300):\n",
    "            x_pred = np.zeros((1, 40+max_len_synop, len(int2char)))\n",
    "#             for t, char in enumerate(sentence):\n",
    "#                 x_pred[0, t+max_len_synop, char2int[char]] = 1.\n",
    "            x_pred[0] = one_hot_encoder(global_synop, sentence)\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int2char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True,mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,patience=1, min_lr=0.001)\n",
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ainvay():\n",
    "    print(len(global_review))\n",
    "    print(len(global_synop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machau(synopsis,review):\n",
    "    fortyforty = (len(review) -36)//5\n",
    "    Y = np.zeros((fortyforty, len(int2char)), dtype=np.float32)\n",
    "    X = np.zeros((fortyforty, 40+max_len_synop, len(int2char)), dtype = np.float32)\n",
    "    for k in range(40, len(review), 5):\n",
    "        l = (int)((k-40)/5)\n",
    "        Y[l] = char_encoding2arr(review[k])\n",
    "        \n",
    "    for i in range(fortyforty):\n",
    "        start_index=i*5;\n",
    "        X[i]=one_hot_encoder(synopsis,review[start_index:start_index+40])\n",
    "    global global_review \n",
    "    global global_synop \n",
    "    global_review = review\n",
    "    global_synop = synopsis\n",
    "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "    filepath = \"weights.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True,mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,patience=1, min_lr=0.001)\n",
    "    callbacks = [print_callback, checkpoint, reduce_lr]\n",
    "    ainvay()\n",
    "#     print(len(global_review))\n",
    "    model.fit(X , Y , batch_size=3 , epochs=3 ,callbacks=callbacks)\n",
    "#     print(Y[0])\n",
    "#     print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "11801\n",
      "Epoch 1/3\n",
      "339/339 [==============================] - 536s 2s/step - loss: 3.3603\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"don't normally watch them. I work at a b\"\n",
      "don't normally watch them. I work at a b ehea o  h e he  he  he aoe  hh aeao oe  te  o  ae  e hhhe he het ee hthhhee ae hh e heo hhe eh  h  th ooaeee o e hhehe a ee oth t het teehe  aeh hhhee heee heeeotethh   ee e h ah on  hahh hhe e heeeeo he a hee e h  at  hh e o  h   a oh oe e  a e a o h  h he heeeaate et  hha  eeae hhh  aae ohaoh h h\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"don't normally watch them. I work at a b\"\n",
      "don't normally watch them. I work at a bhh  ioso od ve ee ehh meeh aee ahhedcewthemhoeahh eahial yna  eo ne heohohttsheeh   Iehhe arehhhe natthh othiihttea enhcetetn th shh aheeh e  ti  eo aa  tih  add o aawaeae heae ooitteohresaeeorh aeoet on oehaotohahhhtht heeoeot tee hh s ate h ehesehte d hteto Kodv tdathah n s h noh eh  de het ose th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"don't normally watch them. I work at a b\"\n",
      "don't normally watch them. I work at a be  itTsoeictha lattho hedothhpa ehInwttlotim ee pwe I cd htea hmvrtod4csthr vantmotdhvnaaciawhhdpeaia.eyahcI)etsoemeerie4its,elwothhsshtknn(a mdcmlaettmiioepetuirametyno onolthshnaguerhkssTdvyteultttene pdsyeewdto*m  oIe Tt att TtdhnsI athg emiwe  ow ch smb ee,o vhtnhhetn hbneaphcascnm pthidlttshta\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"don't normally watch them. I work at a b\"\n",
      "don't normally watch them. I work at a bn syoo4 ate.a-asn ianwWmwyosadr +!Felhh TlgwyhnhoniIehcheGhdb ehr,rrTta Rdicra dvepaueceohrgdtnon atwshdt.eeuhahoeh hkGeaocehii ihwemnmnyeusr mndcc4uobheshaibo!ajhaosaupsetshea ?aah ohinostawete,eh Xrehhcnmli fhrr kmG(an,. meaLhyg csls dotT wisacocot ,di a ln we hnvheiorTyhymVabchhhdiser iwdghostmht\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.36025, saving model to weights.hdf5\n",
      "Epoch 2/3\n",
      "339/339 [==============================] - 482s 1s/step - loss: 2.9356\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"am profoundly moved by this simple and e\"\n",
      "am profoundly moved by this simple and e at t ee d e d ae e d ad d te e ae t aee aa t s aee t ae e o ae  d ae t aee ae t e de t teee  de te aee aa aa ae d ae  ae  ae oe t te de d ae  oe ae d d ot ae d t ae  aant ae wae de ae e aae d aae  aae d te ad  ate  aae d the wae o de ad o de o ao de d e e e c tee de te the de ae re  de ee d te the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"am profoundly moved by this simple and e\"\n",
      "am profoundly moved by this simple and e nene e h he h ot teaum tee ae mdae t ite se ote  th ee aet t  aar d t Ie et oei e ve oe oad ad ar ee th I ne nd few wtne d ene d t ra ou dd o cee d ahe ttee nd ae thnou aset on t a ov cave teac ie aeee aat ade cae -abe  aa e  aatdd o at ie  it an a a ae cat hed ed dtee the ct e eac h aeteo th d te \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"am profoundly moved by this simple and e\"\n",
      "am profoundly moved by this simple and edbthttie oe al `t - mat dvmei oa therdmt tdne wicstowsenNre osongms Ahdor  or rcnut Ro7r nollk thddediney tsi asoe vnadet uwtolIneaIet ddh leh thct drifored rnt ddee aeI et Cintte bedeind wta ro 6veeeavier vie cvien ptoiseded kee fruecgtenyt emwer nyru monit ionr frytn uhaoen ihu5oinlme hnrinnchhee \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"am profoundly moved by this simple and e\"\n",
      "am profoundly moved by this simple and enoo cnYbar ly th -un5 rda ceudkthvee eeê TemTe. Jadits ithdnnnn nI tarciive nllms diu4 biib aRdrsh dc Zrwasonh  adnfnIiehwiinoy diccp K1Ueddn cig 0ceTa etw pessv le kera5lm myecntdy 2err loy ynf ,o maodshcoitdmaspd atheraive boahet \"ipdver8e da deetdono1t seo9ec.ed miithencnl deemtdnnoh , SnAwreed w\n",
      "\n",
      "Epoch 00002: loss improved from 3.36025 to 2.93559, saving model to weights.hdf5\n",
      "Epoch 3/3\n",
      "339/339 [==============================] - 550s 2s/step - loss: 3.0790\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"nd it gets better with every showing.No \"\n",
      "nd it gets better with every showing.No ooo o o ooe o  e o       oo o   oo ee o  oe o   o oooo o  o o o  oo  oo     ooaee o  o d o o  oed o   o ooe o oo   ooo o t  o o o ooho d    oe ooo  o o  oo ooe o     o o  o o o   o ooooe o   oo o o oe o     oe te    o  o oaoe o o oo oo eo to  o  oo o oo  oe oa oe  o  oo  d o oo  o  ooo o    oo o o o\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nd it gets better with every showing.No \"\n",
      "nd it gets better with every showing.No o oed oo o to ootha  e oe  s do oo eo o eodeooooeeht    os osse o do ho et a  o  odl h e eos oo it oon a od t e t ee sdo h oaot o os oadooeee  o ds  otoe  o  oe  oa eoao o ot oooehoh roo o  oda eedr  s odn o  toa  e odoato oo ooaoe  d oooe e  ooon   o oae    o snot o  ad  o oooete   teoda  ete  do  \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nd it gets better with every showing.No \"\n",
      "nd it gets better with every showing.No eedahh ron eersebsrco ee  d rodoere nedthoe t sIh nh dnod cthts de e aoh oa aotdthsdote   oo nes e aos edoto oo   y othathtehd ouo ,heit tsd asht ohhoed aao4eda,h abe t e   ooeagotsth o , eoh d  ase ede te httoehh so oeaeah omderat araaoldt d nost t noteoeo donr rhadu aem hoobideno t oRshee i ed oa \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"nd it gets better with every showing.No \"\n",
      "nd it gets better with every showing.No sdfe  h snasu cd  maaaaoenodo oohfmedeoo o si broee k ess   ateoe ehtwdoa eed ro,onh eoaet do doomeeohmidehdos eeoogniowosw owoaaaltsd uo dohhn  taa oh ed e ptmep ceet   rndtoad a  de tdgaotaot o d  o adhwahoo odowh ed eo   htlg rhi nthedtc eadhd  e theoohe a  atsno tans c see si hshhsoaaeddy seht o\n",
      "\n",
      "Epoch 00003: loss did not improve from 2.93559\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1\n",
    "for chunk in pd.read_csv('preprocessed.tsv', chunksize=chunksize, delimiter='\\t'):\n",
    "    df= pd.DataFrame(chunk)\n",
    "    df['reviews'] = df['reviews'].apply(literal_eval)\n",
    "    for review in df.iloc[0]['reviews']:\n",
    "        machau(df.iloc[0]['synopsis'],review)\n",
    "        break\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity,review,synopsis):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(review) - 40 - 1)\n",
    "    generated = ''\n",
    "    sentence = review[start_index: start_index + 40]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, 40+max_len_synop, len(int2char)))\n",
    "#             for t, char in enumerate(sentence):\n",
    "#                 x_pred[0, t+max_len_synop, char2int[char]] = 1.\n",
    "            x_pred[0] = one_hot_encoder(synopsis, sentence)\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int2char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on? I am not sure - almost everything the o o o oo oo aoo te   oo oo oo    oo o  e o     oo o oaao  ooo o ooe    o oo oo o o ooa e oo      o e oo o  oo e   ooe    e o o ooo o e  o  oae oo  o o o o oo o   o o o o t o o  o   aeod e ooo  o  oo oo oo o oo o   o o     o o oo ao teo  o ooo o o o oo oe  aoa  o  o o  oo  oo a o o ooo  o  o o o o \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(300, 0.2,df.iloc[0]['reviews'][0] ,df.iloc[0]['synopsis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
