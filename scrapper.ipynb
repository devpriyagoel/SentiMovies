{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin \n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "\n",
    "# Download IMDB's Top 250 data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('td.titleColumn')\n",
    "links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "votes = [b.attrs.get('data-value') for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "imdb = []\n",
    "dataset = []\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, 100):\n",
    "    # Seperate movie into: 'place', 'title', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "#            \"year\": year,\n",
    "#             \"place\": place,\n",
    "#             \"star_cast\": crew[index],\n",
    "            \"rating\": ratings[index],\n",
    "#             \"vote\": votes[index],\n",
    "            \"link\": links[index]}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    #print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'],  item['link'])\n",
    "    url =  'http://www.imdb.com'+ item['link'][:17]+'reviews'\n",
    "    driver = webdriver.Chrome('/home/satej/Downloads/chromedriver_linux64/chromedriver')\n",
    "#     driver.get(url)\n",
    "#     la =40\n",
    "#     for loadm in range(0,10):\n",
    "#         labtn = driver.find_element_by_id('load-more-trigger')\n",
    "#         labtn.click()\n",
    "#          wait(driver, 5)\n",
    "#         wait(driver, 15).until(lambda x: len(driver.find_elements_by_css_selector(\"div.lister-item-content\")) > la)\n",
    "#         la+=20\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    for i in range(0,10):\n",
    "        try:\n",
    "            python_button = driver.find_element_by_id('load-more-trigger') #FHSU\n",
    "            python_button.click()\n",
    "        except Exception as e :\n",
    "            break \n",
    "    response=requests.get(url)\n",
    "    \n",
    "    soup= BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "#     source_code = driver.page_source\n",
    "#     soup = BeautifulSoup(source_code, 'lxml')\n",
    "    containers=soup.find_all('div', class_ = 'lister-item-content')\n",
    "    if(len(containers))<200:\n",
    "        continue\n",
    "    \n",
    "    reviews = []\n",
    "    user_ratings=[]\n",
    "    for container in containers:\n",
    "        if container.find('a', class_ = 'title' ) is not None:\n",
    "            title = container.a.text.strip()\n",
    "            \n",
    "            bar = container.find('div', class_ = 'ipl-ratings-bar')\n",
    "            if bar is None:\n",
    "                continue\n",
    "            else:\n",
    "                rating = bar.text.strip()  \n",
    "                content_cont = container.find('div', class_ = 'text show-more__control')\n",
    "                if content_cont is not None:\n",
    "                    content = content_cont.text.strip()\n",
    "                    #content=' '.join(re.split(r'\\t+',content))\n",
    "                    #print(content)\n",
    "                else:\n",
    "                    content_cont = container.find('div', class_ = 'text show-more__control clickable')\n",
    "                    content = content_cont.text.strip()\n",
    "                content = title+' '+content\n",
    "                if len(content)>200:\n",
    "                    content=' '.join(re.split(r'\\t+',content))\n",
    "                    reviews.append(content)\n",
    "                    user_ratings.append(rating)\n",
    "    \n",
    "   \n",
    "    #print(movies) \n",
    "    driver.quit()\n",
    "    url1 =  'http://www.imdb.com'+ item['link'][:17]+'plotsummary'\n",
    "    #print(url1)\n",
    "    response = requests.get(url1)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    containers=soup.find_all('ul', {'id':'plot-synopsis-content'})\n",
    "    synopsis=\"\"\n",
    "    for temp in containers:\n",
    "        for temp1 in temp.find_all('li'):\n",
    "            synopsis=temp1.text.strip()\n",
    "            #print(synopsis)\n",
    "    movies = {\n",
    "        \"title\" : item['movie_title'],\n",
    "        \"rating\" : item['rating'],\n",
    "        \"reviews\": reviews,\n",
    "        \"user_ratings\":user_ratings,\n",
    "        \"synopsis\":synopsis\n",
    "    }\n",
    "    dataset.append(movies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/chart/bottom'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('td.titleColumn')\n",
    "links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "votes = [b.attrs.get('data-value') for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "imdb = []\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, len(movies)):\n",
    "    # Seperate movie into: 'place', 'title', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "#            \"year\": year,\n",
    "#             \"place\": place,\n",
    "#             \"star_cast\": crew[index],\n",
    "            \"rating\": ratings[index],\n",
    "#             \"vote\": votes[index],\n",
    "            \"link\": links[index]}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    #print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'],  item['link'])\n",
    "    url =  'http://www.imdb.com'+ item['link'][:17]+'reviews'\n",
    "    driver = webdriver.Chrome('/home/satej/Downloads/chromedriver_linux64/chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    for i in range(0,10):\n",
    "        try:\n",
    "            python_button = driver.find_element_by_id('load-more-trigger') #FHSU\n",
    "            python_button.click()\n",
    "        except Exception as e :\n",
    "            break \n",
    "    response=requests.get(url)\n",
    "\n",
    "    soup= BeautifulSoup(driver.page_source, 'lxml')\n",
    "    containers=soup.find_all('div', class_ = 'lister-item-content')\n",
    "    if(len(containers))<200:\n",
    "        continue\n",
    "    reviews = []\n",
    "    user_ratings=[]\n",
    "    for container in containers:\n",
    "        if container.find('a', class_ = 'title' ) is not None:\n",
    "            title = container.a.text.strip()\n",
    "            title=' '.join(re.split(r'\\t+',title))\n",
    "            bar = container.find('div', class_ = 'ipl-ratings-bar')\n",
    "            if bar is None:\n",
    "                continue\n",
    "            else:\n",
    "                rating = bar.text.strip()  \n",
    "                content_cont = container.find('div', class_ = 'text show-more__control')\n",
    "                if content_cont is not None:\n",
    "                    content = content_cont.text.strip()\n",
    "                    content=' '.join(re.split(r'\\t+',content))\n",
    "                else:\n",
    "                    content_cont = container.find('div', class_ = 'text show-more__control clickable')\n",
    "                    content = content_cont.text.strip()\n",
    "                content = title+' '+content\n",
    "                if len(content)>200:\n",
    "                    reviews.append(content)\n",
    "                    user_ratings.append(rating)\n",
    "    \n",
    "   \n",
    "    #print(movies) \n",
    "    driver.quit()\n",
    "    url1 =  'http://www.imdb.com'+ item['link'][:17]+'plotsummary'\n",
    "    response = requests.get(url1)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    containers=soup.find_all('ul', {'id':'plot-synopsis-content'})\n",
    "    synopsis=\"\"\n",
    "    for temp in containers:\n",
    "        for temp1 in temp.find_all('li'):\n",
    "            synopsis=temp1.text.strip()\n",
    "            #print(synopsis)\n",
    "    movies = {\n",
    "        \"title\" : item['movie_title'],\n",
    "        \"rating\" : item['rating'],\n",
    "        \"reviews\": reviews,\n",
    "        \"user_ratings\":user_ratings,\n",
    "        \"synopsis\":synopsis\n",
    "    }\n",
    "    dataset.append(movies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/list/ls058813655/?sort=list_order,asc&st_dt=&mode=simple&page=1&ref_=ttls_vw_smp'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# movies = soup.select('td.titleColumn')\n",
    "# links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "# crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "# ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "# votes = [b.attrs.get('data-value') for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "movies = soup.find_all('div',class_='lister-col-wrapper')\n",
    "\n",
    "imdb = []\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, len(movies)):\n",
    "    # Seperate movie into: 'place', 'title', 'year'\n",
    "#     movie_string = movies[index].get_text()\n",
    "#     movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "#     movie_title = movie[len(str(index))+1:-7]\n",
    "    movie_title = movies[index].a.text\n",
    "    rating_temp = movies[index].find('div',class_='col-imdb-rating')\n",
    "    rating = rating_temp.strong.text.strip()\n",
    "    link=movies[index].a.attrs.get('href')\n",
    "#     year = re.search('(.*?)(.∗?)', movie_string).group(1)\n",
    "#     place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "#             \"year\": year,\n",
    "#             \"place\": place,\n",
    "#             \"star_cast\": crew[index],\n",
    "            \"rating\": rating,\n",
    "#             \"vote\": votes[index],\n",
    "            \"link\": link}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    #print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'],  item['link'])\n",
    "    url =  'http://www.imdb.com'+ item['link'][:17]+'reviews'\n",
    "    driver = webdriver.Chrome('/home/satej/Downloads/chromedriver_linux64/chromedriver')\n",
    "   # driver = webdriver.Chrome('/home/udbhav/Downloads/chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    for i in range(0,10):\n",
    "        try:\n",
    "            python_button = driver.find_element_by_id('load-more-trigger') #FHSU\n",
    "            python_button.click()\n",
    "        except Exception as e :\n",
    "            break \n",
    "    response=requests.get(url)\n",
    "\n",
    "    soup= BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    containers=soup.find_all('div', class_ = 'lister-item-content')\n",
    "    if(len(containers))<200:\n",
    "        continue\n",
    "    reviews = []\n",
    "    user_ratings=[]\n",
    "    for container in containers:\n",
    "        if container.find('a', class_ = 'title' ) is not None:\n",
    "            title = container.a.text.strip()\n",
    "            title=' '.join(re.split(r'\\t+',title))\n",
    "            bar = container.find('div', class_ = 'ipl-ratings-bar')\n",
    "            if bar is None:\n",
    "                continue\n",
    "            else:\n",
    "                rating = bar.text.strip()  \n",
    "                content_cont = container.find('div', class_ = 'text show-more__control')\n",
    "                if content_cont is not None:\n",
    "                    content = content_cont.text.strip()\n",
    "                    content=' '.join(re.split(r'\\t+',content))\n",
    "                else:\n",
    "                    content_cont = container.find('div', class_ = 'text show-more__control clickable')\n",
    "                    content = content_cont.text.strip()\n",
    "                content = title+' '+content\n",
    "                if len(content)>200:\n",
    "                    reviews.append(content)\n",
    "                    user_ratings.append(rating)\n",
    "    \n",
    "   \n",
    "    #print(movies) \n",
    "    driver.quit()\n",
    "    url1 =  'http://www.imdb.com'+ item['link'][:17]+'plotsummary'\n",
    "    response = requests.get(url1)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    containers=soup.find_all('ul', {'id':'plot-synopsis-content'})\n",
    "    synopsis=\"\"\n",
    "    for temp in containers:\n",
    "        for temp1 in temp.find_all('li'):\n",
    "            synopsis=temp1.text.strip()\n",
    "            #print(synopsis)\n",
    "    movies = {\n",
    "        \"title\" : item['movie_title'],\n",
    "        \"rating\" : item['rating'],\n",
    "        \"reviews\": reviews,\n",
    "        \"user_ratings\":user_ratings,\n",
    "        \"synopsis\":synopsis\n",
    "    }\n",
    "    dataset.append(movies)\n",
    "    \n",
    "# df=pd.DataFrame(dataset)\n",
    "#print(dataset)\n",
    "# f = open(\"sample1.tsv\", \"w\")\n",
    "# writer = csv.DictWriter(\n",
    "#     f, fieldnames=[\"title\", \"rating\", \"reviews\",\"user_ratings\",\"synopsis\"])\n",
    "# writer.writeheader()\n",
    "# writer.writerows(dataset)\n",
    "# f.close()\n",
    "# df = pd.read_csv('sample1.tsv')\n",
    "# df.to_csv('sample1.tsv',sep='\\t')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/list/ls058982125/?sort=list_order%2Casc&st_dt=&mode=simple&page=1&ref_=ttls_vw_smp&fbclid=IwAR3kTPCg82Cdfloam5U7_cuY8qsSvxnhf2yuKykRBOvx0GUZPQY_s-aXWIg'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# movies = soup.select('td.titleColumn')\n",
    "# links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "# crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "# ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "# votes = [b.attrs.get('data-value') for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "movies = soup.find_all('div',class_='lister-col-wrapper')\n",
    "\n",
    "imdb = []\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, len(movies)):\n",
    "    # Seperate movie into: 'place', 'title', 'year'\n",
    "#     movie_string = movies[index].get_text()\n",
    "#     movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "#     movie_title = movie[len(str(index))+1:-7]\n",
    "    movie_title = movies[index].a.text\n",
    "    rating_temp = movies[index].find('div',class_='col-imdb-rating')\n",
    "    rating = rating_temp.strong.text.strip()\n",
    "    link=movies[index].a.attrs.get('href')\n",
    "#     year = re.search('(.*?)(.∗?)', movie_string).group(1)\n",
    "#     place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "#             \"year\": year,\n",
    "#             \"place\": place,\n",
    "#             \"star_cast\": crew[index],\n",
    "            \"rating\": rating,\n",
    "#             \"vote\": votes[index],\n",
    "            \"link\": link}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    #print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'],  item['link'])\n",
    "    url =  'http://www.imdb.com'+ item['link'][:17]+'reviews'\n",
    "    driver = webdriver.Chrome('/home/satej/Downloads/chromedriver_linux64/chromedriver')\n",
    "   # driver = webdriver.Chrome('/home/udbhav/Downloads/chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    for i in range(0,10):\n",
    "        try:\n",
    "            python_button = driver.find_element_by_id('load-more-trigger') #FHSU\n",
    "            python_button.click()\n",
    "        except Exception as e :\n",
    "            break \n",
    "    response=requests.get(url)\n",
    "\n",
    "    soup= BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    containers=soup.find_all('div', class_ = 'lister-item-content')\n",
    "    if(len(containers))<200:\n",
    "        continue\n",
    "    reviews = []\n",
    "    user_ratings=[]\n",
    "    for container in containers:\n",
    "        if container.find('a', class_ = 'title' ) is not None:\n",
    "            title = container.a.text.strip()\n",
    "            title=' '.join(re.split(r'\\t+',title))\n",
    "            bar = container.find('div', class_ = 'ipl-ratings-bar')\n",
    "            if bar is None:\n",
    "                continue\n",
    "            else:\n",
    "                rating = bar.text.strip()  \n",
    "                content_cont = container.find('div', class_ = 'text show-more__control')\n",
    "                if content_cont is not None:\n",
    "                    content = content_cont.text.strip()\n",
    "                    content=' '.join(re.split(r'\\t+',content))\n",
    "                else:\n",
    "                    content_cont = container.find('div', class_ = 'text show-more__control clickable')\n",
    "                    content = content_cont.text.strip()\n",
    "                content = title+' '+content\n",
    "                if len(content)>200:\n",
    "                    reviews.append(content)\n",
    "                    user_ratings.append(rating)\n",
    "    \n",
    "   \n",
    "    #print(movies) \n",
    "    driver.quit()\n",
    "    url1 =  'http://www.imdb.com'+ item['link'][:17]+'plotsummary'\n",
    "    response = requests.get(url1)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    containers=soup.find_all('ul', {'id':'plot-synopsis-content'})\n",
    "    synopsis=\"\"\n",
    "    for temp in containers:\n",
    "        for temp1 in temp.find_all('li'):\n",
    "            synopsis=temp1.text.strip()\n",
    "            #print(synopsis)\n",
    "    movies = {\n",
    "        \"title\" : item['movie_title'],\n",
    "        \"rating\" : item['rating'],\n",
    "        \"reviews\": reviews,\n",
    "        \"user_ratings\":user_ratings,\n",
    "        \"synopsis\":synopsis\n",
    "    }\n",
    "    dataset.append(movies)\n",
    "    \n",
    "# df=pd.DataFrame(dataset)\n",
    "#print(dataset)\n",
    "# f = open(\"sample1.tsv\", \"w\")\n",
    "# writer = csv.DictWriter(\n",
    "#     f, fieldnames=[\"title\", \"rating\", \"reviews\",\"user_ratings\",\"synopsis\"])\n",
    "# writer.writeheader()\n",
    "# writer.writerows(dataset)\n",
    "# f.close()\n",
    "# df = pd.read_csv('sample1.tsv')\n",
    "# df.to_csv('sample1.tsv',sep='\\t')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                rating                                            reviews  \\\n",
      "0    9.217056355636828  [Tied for the best movie I have ever seen Why ...   \n",
      "1     9.15549387313404  [For Me, This Is The Definitive Film This isn'...   \n",
      "2    8.990103004076515  [Breathtaking in its scope and tragic grandeur...   \n",
      "3    8.960429040207305  [Film surpasses the hype We've been subjected ...   \n",
      "4    8.913663475361679  [Excellent An excellent courtroom drama with a...   \n",
      "5    8.900143150990464  [Bring me the heads of Hitler, Himmler, Eichma...   \n",
      "6    8.870293032838246  [Unprecedented. Peter Jackson has done it.  He...   \n",
      "7    8.856931972471033  [The masterpiece without a message One of the ...   \n",
      "8    8.823059848656573  [An epic Sergio Leone's masterpiece the best s...   \n",
      "9    8.774161714395659  [A unique film Fight Club is one of the most u...   \n",
      "10   8.769365161115452  [An absolutely incredible film! Simply incredi...   \n",
      "11   8.734974493610359  [Life's Lessons in one Movie... When I first s...   \n",
      "12   8.720922592745294  [Excellent timeless classic, the best sequel o...   \n",
      "13   8.719462221909286  [Matrix but in dreamworld? Nah. I'd like to ke...   \n",
      "14   8.689657774173808  [An Epic in every sense of the word. Peter Jac...   \n",
      "15   8.663950144275267  [Both uplifting and disheartening, sometimes b...   \n",
      "16   8.657649985877343  [In one word: perfection Needs to be seen to b...   \n",
      "17   8.645031241770011  [Welcome to the Real World. Without a doubt on...   \n",
      "18    8.62723151695408  [Required Viewing The archetypal action film, ...   \n",
      "19   8.599436700040338  [\"Se7en is well crafted and ingeniously clever...   \n",
      "20   8.598566581385878  [Stunning - in every sense of the word Cidade ...   \n",
      "21    8.59616560829362  [A classics, phenomenal and arguably the best ...   \n",
      "22   8.588317482480866  [Dr.Lecter, I'd like to see you again... The S...   \n",
      "23   8.576810621111498  [Who are you, really? Honestly, I don't think ...   \n",
      "24   8.568454871622079  [The Best Movie I've seen for a long long time...   \n",
      "25   8.542733790101346  [Strange, touching and beautiful Personally, I...   \n",
      "26   8.541253921881783  [War is hell, and \"Saving Private Ryan\" peeks ...   \n",
      "27   8.538930378724444  [The most enjoyment you'll have seeing a movie...   \n",
      "28   8.531096092680697  [Gripping story with well-crafted characters L...   \n",
      "29   8.513837025306968  [As wonderful as the book! \"The Green Mile\" is...   \n",
      "..                 ...                                                ...   \n",
      "327                5.2  [Silly Teen Adventure When Earth is attacked b...   \n",
      "328                6.9  [A great Fantasy flick I didn't have any expec...   \n",
      "329                6.6  [Lacks intelligence of all previous Lacklustre...   \n",
      "330                6.5  [The best bits are all in the trailer The trai...   \n",
      "331                  8  [\"I don't know when to quit!\" Well, I'll tell ...   \n",
      "332                5.9  [Know what you're getting into and you can hav...   \n",
      "333                4.7  [Funny but not focused If you're a fan of the ...   \n",
      "334                6.2  [The problem with Colossal is not Colossal's p...   \n",
      "335                7.3  [John Wick meets Good Will Hunting It's as if ...   \n",
      "336                5.8  [Rated on pure entertainment factor This movie...   \n",
      "337                7.2  [Unfortunately the ending ruined an otherwise ...   \n",
      "338                6.6  [wish all indies were this good... and I can s...   \n",
      "339                6.2  [Lousy, Loud, Unfunny Garbage If your idea of ...   \n",
      "340                6.7  [Could've been a great film. Despite some intr...   \n",
      "341                6.3  [This plot has more holes in it than Swiss che...   \n",
      "342                7.1  [Everyone at the Rig SIte have the right to ST...   \n",
      "343                6.9  [A 6 if you didn't see the original a 4 if you...   \n",
      "344                  6  [People are being too hard on this good movie,...   \n",
      "345                7.3  [Snowden was a Patriot I don't understand peop...   \n",
      "346                  7  [Swiss Army Man - Movie Review \"If my best fri...   \n",
      "347                5.6  [Hands down the WORST editing in the history o...   \n",
      "348                7.6  [Beautiful in every sense, Moana will soften e...   \n",
      "349                7.3  [13 Hours that didn't have to be... For the mo...   \n",
      "350                7.4  [A raucous buddy comedy",
      " and stuff. I'm sure t...   \n",
      "351                7.9  [No CGI overkill, just some fine acting and di...   \n",
      "352                7.5  [\"I'll. Be. Right. Here.\" The worst thing abou...   \n",
      "353                6.2  [Visually stunning, but without substance In a...   \n",
      "354                  6  [The Opposite of White-washing I'm Chinese and...   \n",
      "355                6.3  [Excellent horror movie! I personally think th...   \n",
      "356                6.2  [A Bad Movie for Moms Five minutes into Bad Mo...   \n",
      "\n",
      "                                              synopsis  \\\n",
      "0    In 1947, Andy Dufresne (Tim Robbins), a banker...   \n",
      "1    In late summer 1945, guests are gathered for t...   \n",
      "2    The Godfather Part II presents two parallel st...   \n",
      "3    The movie begins with a gang of men with clown...   \n",
      "4    In a New York City courthouse, an eighteen-yea...   \n",
      "5    The relocation of Polish Jews from surrounding...   \n",
      "6    In the opening scene, a flashback, two hobbits...   \n",
      "7    Late one morning in the Hawthorne Grill, a res...   \n",
      "8    The film tells the story of three men who purs...   \n",
      "9    We back out of the webbing of neurons and brai...   \n",
      "10   The prologue, spoken by Galadriel, shows the D...   \n",
      "11   The film begins with a feather falling to the ...   \n",
      "12   In this sequel to the previous Star Wars movie...   \n",
      "13   A young man, exhausted and delirious, washes u...   \n",
      "14   The film begins with a flashback to the first ...   \n",
      "15   In 1963 Oregon, Randle Patrick McMurphy (Nicho...   \n",
      "16   The film opens with three men driving in their...   \n",
      "17   The screen is filled with green, cascading cod...   \n",
      "18   A gang of marauding bandits approaches a mount...   \n",
      "19   In an unidentified city of constant rain and u...   \n",
      "20   Taking place over the course of over two decad...   \n",
      "21   Note: Italicized paragraphs describe scenes ad...   \n",
      "22   Promising FBI Academy student Clarice Starling...   \n",
      "23   This movie is about a divine intervention by a...   \n",
      "24   Set in late 1930s Arezzo, Italy, Jewish man an...   \n",
      "25   Ten-year-old Chihiro (voice: Daveigh Chase in ...   \n",
      "26   An American flag back-lighted by the afternoon...   \n",
      "27   On the deck of a ship in San Pedro, California...   \n",
      "28   Léon (Jean Reno) is a hitman (or \"cleaner\" as ...   \n",
      "29   The movie opens with a group of people running...   \n",
      "..                                                 ...   \n",
      "327  In the opening scene, Cassie Sullivan (Chloe G...   \n",
      "328  For ages in the region of Middle Earth, humans...   \n",
      "329  The film starts with a brief recap of David We...   \n",
      "330  The film opens in New York City, where a littl...   \n",
      "331  In the town of Bunnyburrow, 9 year old bunny, ...   \n",
      "332  Jason Kelly (Zac Efron) is at a funeral for hi...   \n",
      "333  The film starts in Rome, where two assassins o...   \n",
      "334  The film opens in Seoul, South Korea. A little...   \n",
      "335  The film begins with a man walking through a b...   \n",
      "336  Set in an alternate England of the early 1800s...   \n",
      "337  In the opening scene, a woman named Michelle (...   \n",
      "338  The glow of a television set illuminates an ot...   \n",
      "339  The film opens at the supermarket Shopwell's o...   \n",
      "340  Florida teen Jake Portman (Asa Butterfield) fe...   \n",
      "341  Nancy Adams (Blake Lively) is a college medica...   \n",
      "342  The film opens with actual recorded audio of a...   \n",
      "343  In this remake of the 1960 film of the same na...   \n",
      "344  In the opening scene, we see Greta Evans (Laur...   \n",
      "345  Hong Kong - Monday June 3, 2013Documentary fil...   \n",
      "346  The film starts with shots of several items fl...   \n",
      "347  The film opens with Alice (Milla Jovovich) exp...   \n",
      "348  Using a tapa cloth to animate her story, Gramm...   \n",
      "349  The opening text states that there were over 2...   \n",
      "350  The setting is the city of Los Angeles, Califo...   \n",
      "351  The film starts with the voice of Dr. Louise B...   \n",
      "352  The film begins with 12-year-old Conor (Lewis ...   \n",
      "353  The movie opens up with beautiful blond teenag...   \n",
      "354  The opening text states that the Great Wall of...   \n",
      "355  The film opens with Paul (Billy Burke) skyping...   \n",
      "356  Set in the Chicago suburbs, Amy Mitchell (Mila...   \n",
      "\n",
      "                                                 title  \\\n",
      "0                             The Shawshank Redemption   \n",
      "1                                        The Godfather   \n",
      "2                               The Godfather: Part II   \n",
      "3                                      The Dark Knight   \n",
      "4                                         12 Angry Men   \n",
      "5                                     Schindler's List   \n",
      "6        The Lord of the Rings: The Return of the King   \n",
      "7                                         Pulp Fiction   \n",
      "8                      Il buono, il brutto, il cattivo   \n",
      "9                                           Fight Club   \n",
      "10   The Lord of the Rings: The Fellowship of the Ring   \n",
      "11                                        Forrest Gump   \n",
      "12      Star Wars: Episode V - The Empire Strikes Back   \n",
      "13                                           Inception   \n",
      "14               The Lord of the Rings: The Two Towers   \n",
      "15                     One Flew Over the Cuckoo's Nest   \n",
      "16                                          Goodfellas   \n",
      "17                                          The Matrix   \n",
      "18                                Shichinin no samurai   \n",
      "19                                               Se7en   \n",
      "20                                      Cidade de Deus   \n",
      "21                                           Star Wars   \n",
      "22                            The Silence of the Lambs   \n",
      "23                               It's a Wonderful Life   \n",
      "24                                     La vita è bella   \n",
      "25                       Sen to Chihiro no kamikakushi   \n",
      "26                                 Saving Private Ryan   \n",
      "27                                  The Usual Suspects   \n",
      "28                                                Léon   \n",
      "29                                      The Green Mile   \n",
      "..                                                 ...   \n",
      "327                                       The 5th Wave   \n",
      "328                                           Warcraft   \n",
      "329                                       Jason Bourne   \n",
      "330                            The Secret Life of Pets   \n",
      "331                                           Zootopia   \n",
      "332                                      Dirty Grandpa   \n",
      "333                                        Zoolander 2   \n",
      "334                                           Colossal   \n",
      "335                                     The Accountant   \n",
      "336                    Pride and Prejudice and Zombies   \n",
      "337                                10 Cloverfield Lane   \n",
      "338                                   Midnight Special   \n",
      "339                                      Sausage Party   \n",
      "340        Miss Peregrine's Home for Peculiar Children   \n",
      "341                                       The Shallows   \n",
      "342                                  Deepwater Horizon   \n",
      "343                              The Magnificent Seven   \n",
      "344                                            The Boy   \n",
      "345                                            Snowden   \n",
      "346                                     Swiss Army Man   \n",
      "347                   Resident Evil: The Final Chapter   \n",
      "348                                              Moana   \n",
      "349                                           13 Hours   \n",
      "350                                      The Nice Guys   \n",
      "351                                            Arrival   \n",
      "352                                    A Monster Calls   \n",
      "353                                     The Neon Demon   \n",
      "354                                     The Great Wall   \n",
      "355                                         Lights Out   \n",
      "356                                           Bad Moms   \n",
      "\n",
      "                                          user_ratings  \n",
      "0    [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "1    [10/10, 10/10, 10/10, 10/10, 10/10, 9/10, 10/1...  \n",
      "2    [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "3    [10/10, 10/10, 10/10, 10/10, 10/10, 9/10, 10/1...  \n",
      "4    [10/10, 10/10, 9/10, 10/10, 10/10, 10/10, 9/10...  \n",
      "5    [10/10, 10/10, 9/10, 10/10, 10/10, 10/10, 9/10...  \n",
      "6    [10/10, 10/10, 10/10, 10/10, 9/10, 10/10, 10/1...  \n",
      "7    [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "8    [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "9    [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "10   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "11   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "12   [10/10, 10/10, 10/10, 9/10, 10/10, 10/10, 10/1...  \n",
      "13   [9/10, 10/10, 10/10, 9/10, 7/10, 10/10, 10/10,...  \n",
      "14   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "15   [10/10, 10/10, 10/10, 10/10, 10/10, 9/10, 10/1...  \n",
      "16   [10/10, 10/10, 10/10, 10/10, 9/10, 10/10, 10/1...  \n",
      "17   [10/10, 10/10, 10/10, 10/10, 9/10, 10/10, 10/1...  \n",
      "18   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "19   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "20   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "21   [10/10, 10/10, 10/10, 10/10, 9/10, 10/10, 10/1...  \n",
      "22   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "23   [10/10, 10/10, 10/10, 10/10, 9/10, 10/10, 10/1...  \n",
      "24   [10/10, 9/10, 10/10, 9/10, 10/10, 9/10, 10/10,...  \n",
      "25   [10/10, 10/10, 10/10, 9/10, 10/10, 10/10, 10/1...  \n",
      "26   [9/10, 10/10, 10/10, 10/10, 10/10, 10/10, 9/10...  \n",
      "27   [10/10, 10/10, 9/10, 10/10, 9/10, 10/10, 10/10...  \n",
      "28   [10/10, 10/10, 10/10, 9/10, 10/10, 10/10, 10/1...  \n",
      "29   [10/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/...  \n",
      "..                                                 ...  \n",
      "327  [5/10, 4/10, 5/10, 1/10, 2/10, 3/10, 5/10, 2/1...  \n",
      "328  [10/10, 10/10, 10/10, 10/10, 10/10, 8/10, 10/1...  \n",
      "329  [5/10, 1/10, 4/10, 7/10, 1/10, 7/10, 4/10, 1/1...  \n",
      "330  [4/10, 7/10, 4/10, 6/10, 9/10, 4/10, 5/10, 9/1...  \n",
      "331  [8/10, 9/10, 8/10, 10/10, 10/10, 10/10, 10/10,...  \n",
      "332  [6/10, 6/10, 9/10, 7/10, 6/10, 10/10, 10/10, 2...  \n",
      "333  [6/10, 4/10, 2/10, 7/10, 3/10, 8/10, 2/10, 1/1...  \n",
      "334  [7/10, 7/10, 8/10, 8/10, 8/10, 4/10, 3/10, 3/1...  \n",
      "335  [8/10, 10/10, 8/10, 9/10, 9/10, 10/10, 7/10, 7...  \n",
      "336  [8/10, 8/10, 9/10, 10/10, 9/10, 9/10, 8/10, 6/...  \n",
      "337  [7/10, 5/10, 7/10, 5/10, 9/10, 9/10, 5/10, 8/1...  \n",
      "338  [9/10, 8/10, 6/10, 3/10, 4/10, 8/10, 9/10, 9/1...  \n",
      "339  [1/10, 1/10, 3/10, 2/10, 8/10, 2/10, 1/10, 1/1...  \n",
      "340  [5/10, 8/10, 5/10, 4/10, 4/10, 3/10, 8/10, 1/1...  \n",
      "341  [4/10, 7/10, 7/10, 3/10, 3/10, 3/10, 3/10, 4/1...  \n",
      "342  [10/10, 9/10, 7/10, 8/10, 7/10, 7/10, 6/10, 8/...  \n",
      "343  [5/10, 2/10, 3/10, 3/10, 2/10, 6/10, 4/10, 1/1...  \n",
      "344  [8/10, 8/10, 9/10, 7/10, 9/10, 10/10, 9/10, 6/...  \n",
      "345  [9/10, 10/10, 8/10, 9/10, 10/10, 10/10, 9/10, ...  \n",
      "346  [8/10, 10/10, 8/10, 10/10, 8/10, 10/10, 10/10,...  \n",
      "347  [2/10, 1/10, 2/10, 2/10, 3/10, 3/10, 1/10, 1/1...  \n",
      "348  [9/10, 9/10, 5/10, 10/10, 8/10, 9/10, 8/10, 9/...  \n",
      "349  [8/10, 8/10, 9/10, 10/10, 8/10, 9/10, 9/10, 10...  \n",
      "350  [8/10, 9/10, 9/10, 8/10, 7/10, 9/10, 3/10, 9/1...  \n",
      "351  [9/10, 10/10, 10/10, 10/10, 10/10, 10/10, 10/1...  \n",
      "352  [9/10, 9/10, 9/10, 10/10, 9/10, 10/10, 10/10, ...  \n",
      "353  [6/10, 2/10, 8/10, 8/10, 9/10, 9/10, 1/10, 1/1...  \n",
      "354  [10/10, 8/10, 8/10, 10/10, 7/10, 4/10, 9/10, 1...  \n",
      "355  [10/10, 6/10, 8/10, 2/10, 4/10, 4/10, 5/10, 8/...  \n",
      "356  [3/10, 2/10, 2/10, 1/10, 1/10, 1/10, 3/10, 1/1...  \n",
      "\n",
      "[357 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "url='https://www.imdb.com/list/ls058938704/?sort=list_order%2Casc&st_dt=&mode=simple&page=1&ref_=ttls_vw_smp&fbclid=IwAR03yeKZWFJpudJBzkMo2pwZ1UAgqG5BXzHn9jbr55WwzEi-TmZn-fCqdGY'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# movies = soup.select('td.titleColumn')\n",
    "# links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "# crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "# ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "# votes = [b.attrs.get('data-value') for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "movies = soup.find_all('div',class_='lister-col-wrapper')\n",
    "\n",
    "imdb = []\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, len(movies)):\n",
    "    # Seperate movie into: 'place', 'title', 'year'\n",
    "#     movie_string = movies[index].get_text()\n",
    "#     movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "#     movie_title = movie[len(str(index))+1:-7]\n",
    "    movie_title = movies[index].a.text\n",
    "    rating_temp = movies[index].find('div',class_='col-imdb-rating')\n",
    "    rating = rating_temp.strong.text.strip()\n",
    "    link=movies[index].a.attrs.get('href')\n",
    "#     year = re.search('(.*?)(.∗?)', movie_string).group(1)\n",
    "#     place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "#             \"year\": year,\n",
    "#             \"place\": place,\n",
    "#             \"star_cast\": crew[index],\n",
    "            \"rating\": rating,\n",
    "#             \"vote\": votes[index],\n",
    "            \"link\": link}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    #print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'],  item['link'])\n",
    "    url =  'http://www.imdb.com'+ item['link'][:17]+'reviews'\n",
    "    driver = webdriver.Chrome('/home/satej/Downloads/chromedriver_linux64/chromedriver')\n",
    "   # driver = webdriver.Chrome('/home/udbhav/Downloads/chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    for i in range(0,10):\n",
    "        try:\n",
    "            python_button = driver.find_element_by_id('load-more-trigger') #FHSU\n",
    "            python_button.click()\n",
    "        except Exception as e :\n",
    "            break \n",
    "    response=requests.get(url)\n",
    "\n",
    "    soup= BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    containers=soup.find_all('div', class_ = 'lister-item-content')\n",
    "    if(len(containers))<200:\n",
    "        continue\n",
    "    reviews = []\n",
    "    user_ratings=[]\n",
    "    for container in containers:\n",
    "        if container.find('a', class_ = 'title' ) is not None:\n",
    "            title = container.a.text.strip()\n",
    "            title=' '.join(re.split(r'\\t+',title))\n",
    "            bar = container.find('div', class_ = 'ipl-ratings-bar')\n",
    "            if bar is None:\n",
    "                continue\n",
    "            else:\n",
    "                rating = bar.text.strip()  \n",
    "                content_cont = container.find('div', class_ = 'text show-more__control')\n",
    "                if content_cont is not None:\n",
    "                    content = content_cont.text.strip()\n",
    "                    content=' '.join(re.split(r'\\t+',content))\n",
    "                else:\n",
    "                    content_cont = container.find('div', class_ = 'text show-more__control clickable')\n",
    "                    content = content_cont.text.strip()\n",
    "                content = title+' '+content\n",
    "                if len(content)>200:\n",
    "                    reviews.append(content)\n",
    "                    user_ratings.append(rating)\n",
    "    \n",
    "   \n",
    "    #print(movies) \n",
    "    driver.quit()\n",
    "    url1 =  'http://www.imdb.com'+ item['link'][:17]+'plotsummary'\n",
    "    response = requests.get(url1)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    containers=soup.find_all('ul', {'id':'plot-synopsis-content'})\n",
    "    synopsis=\"\"\n",
    "    for temp in containers:\n",
    "        for temp1 in temp.find_all('li'):\n",
    "            synopsis=temp1.text.strip()\n",
    "            #print(synopsis)\n",
    "    movies = {\n",
    "        \"title\" : item['movie_title'],\n",
    "        \"rating\" : item['rating'],\n",
    "        \"reviews\": reviews,\n",
    "        \"user_ratings\":user_ratings,\n",
    "        \"synopsis\":synopsis\n",
    "    }\n",
    "    dataset.append(movies)\n",
    "    \n",
    "df=pd.DataFrame(dataset)\n",
    "#print(dataset)\n",
    "# f = open(\"sample1.tsv\", \"w\")\n",
    "# writer = csv.DictWriter(\n",
    "#     f, fieldnames=[\"title\", \"rating\", \"reviews\",\"user_ratings\",\"synopsis\"])\n",
    "# writer.writeheader()\n",
    "# writer.writerows(dataset)\n",
    "# f.close()\n",
    "# df = pd.read_csv('sample1.tsv')\n",
    "df.to_csv('sample1.tsv',sep='\\t')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
